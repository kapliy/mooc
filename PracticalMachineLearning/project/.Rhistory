library('caret')   # learning
library('rattle')  # plotting of trees
# parallel computing to utilize multiple cores
library(doParallel)
registerDoParallel(cores=2)
# load the data, interpreting certain strings as NaNs:
nans <- c("NA", "", "#DIV/0!")
pml_training <- read.csv('pml-training.csv', header=T, na.strings=nans)
pml_testing <- read.csv('pml-testing.csv', header=T, na.strings=nans)
# 1. A. drop username and timestamp columns (they are just metadata collected along with actual sensor data)
#    B. filter out the columns where ALL values are NaNs
cols_metadata = c('user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp')
cols_all <- colnames(pml_training)
cols_all <- cols_all[!(cols_all %in% cols_metadata)]  # filter out metadata columns
# new approach: drop columns that have ANY N/A values
cols_allna <- cols_all[colSums(is.na(pml_training))==0]
## cols_allna <- cols_all[colSums(is.na(pml_training))==nrow(pml_training)]
print(cat('Dropped', length(cols_allna), 'columns (only NA values):', cols_allna))
cols <- cols_all[!(cols_all %in% cols_allna)]  # filter out all-NA columns
# 2. filter out columns with no variable (these provide little separation power between outcomes)
nzv <- nearZeroVar(pml_training[, cols], saveMetrics = T, allowParallel=T)
cols_novar <- cols[nzv[, 'zeroVar']]
print(cat('Dropped', length(cols_novar), 'columns (zero variance):', cols_novar))
cols <- cols[!(cols %in% cols_novar)]
# 3. drop bad columns from both the training and testing (aka benchmark) datasets
ftrain <- pml_training[, colnames(pml_training) %in% cols]
fbench <- pml_testing[, colnames(pml_testing) %in% c(cols, 'problem_id')]
# 4. separate numeric and factor columns (for later use)
cols_numeric_bool <- sapply(cols, function(col){is.numeric(ftrain[,col])})
cols_numeric <- cols[cols_numeric_bool == T]
cols_factor <- cols[-cols_numeric_bool == F]
########### Visualization ##############
# dependent variables
qplot(ftrain$classe, geom="histogram", main='Distribution of dependent variable', xlab='classe', ylab='Counts')
ftrain
ftrain[1,]
library('caret')   # learning
library('rattle')  # plotting of trees
# parallel computing to utilize multiple cores
library(doParallel)
registerDoParallel(cores=2)
# load the data, interpreting certain strings as NaNs:
nans <- c("NA", "", "#DIV/0!")
pml_training <- read.csv('pml-training.csv', header=T, na.strings=nans)
pml_testing <- read.csv('pml-testing.csv', header=T, na.strings=nans)
# 1. A. drop username and timestamp columns (they are just metadata collected along with actual sensor data)
#    B. filter out the columns where ALL values are NaNs
cols_metadata = c('user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp')
cols_all <- colnames(pml_training)
cols_all <- cols_all[!(cols_all %in% cols_metadata)]  # filter out metadata columns
# new approach: drop columns that have ANY N/A values
cols_anyna <- cols_all[colSums(is.na(pml_training))>0]
cols_allna <- cols_all[colSums(is.na(pml_training))==nrow(pml_training)]
print(cat('Dropped', length(cols_anyna), 'columns (only NA values):', cols_anyna))
cols <- cols_all[!(cols_all %in% cols_anyna)]  # filter out all-NA columns
cols
colnames(pml_training) %in% cols
# 2. filter out columns with no variable (these provide little separation power between outcomes)
nzv <- nearZeroVar(pml_training[, cols], saveMetrics = T, allowParallel=T)
cols_novar <- cols[nzv[, 'zeroVar']]
print(cat('Dropped', length(cols_novar), 'columns (zero variance):', cols_novar))
cols <- cols[!(cols %in% cols_novar)]
# 3. drop bad columns from both the training and testing (aka benchmark) datasets
ftrain <- pml_training[, colnames(pml_training) %in% cols]
fbench <- pml_testing[, colnames(pml_testing) %in% c(cols, 'problem_id')]
ftrain
ftrain[1,]
ftrain[1, 'kurtosis_picth_arm']
is.na(ftrain[1, 'kurtosis_picth_arm'])
colSums(is.na(ftrain))
cols_anyna
is.na(ftrain[1, 'kurtosis_picth_arm'])
cols_anyna
kurtosis_picth_arm %in% cols_anyna
'kurtosis_picth_arm' %in% cols_anyna
library('caret')   # learning
library('rattle')  # plotting of trees
# parallel computing to utilize multiple cores
library(doParallel)
registerDoParallel(cores=2)
# load the data, interpreting certain strings as NaNs:
nans <- c("NA", "", "#DIV/0!")
pml_training <- read.csv('pml-training.csv', header=T, na.strings=nans)
pml_testing <- read.csv('pml-testing.csv', header=T, na.strings=nans)
# 1. A. drop username and timestamp columns (they are just metadata collected along with actual sensor data)
#    B. filter out the columns where ALL values are NaNs
cols_all <- colnames(pml_training)
cols_metadata = c('user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp')
print(cat('Drop', length(cols_metadata), 'columns (irrelevant metadata):', cols_metadata))
cols_anyna <- cols_all[colSums(is.na(pml_training))>0]
print(cat('Dropped', length(cols_anyna), 'columns (NA values):', cols_anyna))
cols <- cols_all[!(cols_all %in% c(cols_metadata,cols_anyna))]
# 2. filter out columns with no variable (these provide little separation power between outcomes)
nzv <- nearZeroVar(pml_training[, cols], saveMetrics = T, allowParallel=T)
cols_novar <- cols[nzv[, 'zeroVar']]
print(cat('Dropped', length(cols_novar), 'columns (zero variance):', cols_novar))
cols <- cols[!(cols %in% cols_novar)]
# 3. drop bad columns from both the training and testing (aka benchmark) datasets
ftrain <- pml_training[, colnames(pml_training) %in% cols]
fbench <- pml_testing[, colnames(pml_testing) %in% c(cols, 'problem_id')]
# 4. separate numeric and factor columns (for later use)
cols_numeric_bool <- sapply(cols, function(col){is.numeric(ftrain[,col])})
cols_numeric <- cols[cols_numeric_bool == T]
cols_factor <- cols[-cols_numeric_bool == F]
# dummy vars for vactor variables [should not be necessary]
# modDummy <- dummyVars(classe ~ ., data = ftrain)
########### Visualization ##############
# dependent variables
qplot(ftrain$classe, geom="histogram", main='Distribution of dependent variable', xlab='classe', ylab='Counts')
# split full dataset (ftrain) into train/val/test using 60-20-20 fractions
set.seed(1)
p6 <- createDataPartition(ftrain$classe, p=0.6, list=F)
train <- ftrain[p6, ]  # 60%
tv <- ftrain[-p6, ]    # 40%
p5 <- createDataPartition(tv$classe, p=0.5, list=F)
val <- tv[p5, ]       # 0.5*40% = 20%
test <- tv[-p5, ]     # 0.5*40% = 20%
get_model_01 <- function(save_mode = F) {
# random forest
name <- 'model.v01.rf.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rf', preProcess=c("center", "scale", "pca"),
allowParallel=T, trControl = trainControl(method = "cv", number=10))
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_02 <- function(save_mode = F) {
# decision tree
name <- 'model.v02.rpart.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rpart', preProcess=c("center", "scale", "pca"),
allowParallel=T)
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_03 <- function(save_mode = F) {
# boosted decision tree
name <- 'model.v03.gbm.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='gbm', preProcess=c("center", "scale", "pca"),
allowParallel=T, trControl = trainControl(method = "cv", number=5))
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
train_all_models <- function() {
get_model_01(save_mode=T)
get_model_02(save_mode=T)
get_model_03(save_mode=T)
}
train_all_models()
mod1 <- get_model_01()
mod1
predict(mod1, ftrain)
predict(mod1, fbench)
predict(mod1, fval)
predict(mod1, val)
confusionMatrix(predict(mod1, val), val$classe)
confusionMatrix(predict(mod1, val), val$classe)
pred <- predict(mod1, newdata=fbench)
print(pred)
get_model_01 <- function(save_mode = F) {
# random forest
name <- 'model.v01.rf.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rf', # preProcess=c("center", "scale", "pca"),
allowParallel=T, trControl = trainControl(method = "cv", number=10))
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_02 <- function(save_mode = F) {
# decision tree
name <- 'model.v02.rpart.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rpart', # preProcess=c("center", "scale", "pca"),
allowParallel=T)
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
#fancyRpartPlot(mod$finalModel)
mod
}
get_model_03 <- function(save_mode = F) {
# boosted decision tree
name <- 'model.v03.gbm.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='gbm', # preProcess=c("center", "scale", "pca"),
allowParallel=T, trControl = trainControl(method = "cv", number=5))
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
train_all_models <- function() {
get_model_01(save_mode=T)
get_model_02(save_mode=T)
get_model_03(save_mode=T)
}
train_all_models()
mod1 <- get_model_01()
predict(mod1, fbench)
