T(cols_numeric_bool)
?transpose
??transpose
cols_numeric_bool.t
cols_numeric_bool
class(cols_numeric_bool)
cols_numeric_bool
cols_numeric_bool[1]
cols_numeric_bool[2]
cols_numeric_bool[3]
cols[cols_numeric_bool[3]]
cols_numeric_bool[3]
cols_numeric_bool == T
cols[cols_numeric_bool == T]
cols[cols_numeric_bool == F]
cols <- colnames(fdata)
cols_numeric_bool <- sapply(cols, function(col){is.numeric(fdata[,col])})
cols_numeric <- cols[cols_numeric_bool == T]
cols_factor <- cols[-cols_numeric_bool == F]
cols_numeric
cols_factor
fdata[, "cvtd_timestamp"]
library(lubridate)
is.Date
is.Date(fdata[, "cvtd_timestamp"])
is.Date(fdata[1, "cvtd_timestamp"])
fdata[1, "cvtd_timestamp"]
fdata[1, "cvtd_timestamp"]
fdata[1, "cvtd_timestamp"]
cols_factor <- cols[-cols_numeric_bool == F]
cols_factor
fdata[,'kurtosis_picth_arm']
as.numeric(fdata[,'kurtosis_picth_arm'])
fdata[,'kurtosis_picth_arm']
as.numeric(fdata[,'kurtosis_picth_arm'])
as.numeric(fdata[,'kurtosis_picth_arm'])
fdata[,'kurtosis_picth_arm']
fdata[1,'kurtosis_picth_arm']
fdata[2,'kurtosis_picth_arm']
fdata[4,'kurtosis_picth_arm']
class(fdata[4,'kurtosis_picth_arm'])
fdata[4,'kurtosis_picth_arm']
cols_factor <- cols[-cols_numeric_bool == F]
col_factor
cols_factor
cols[c(1:6)]
?read.csv
?read.csv
bench
bench[1,]
bench[2,]
bench[1,]
dim(bench)
nans <- c("NA", "", "#DIV/0!")
bench <- read.csv('pml-testing.csv', header=T, na.strings=nans)
fdata <- read.csv('pml-training.csv', header=T, na.strings=nans)
fdata
grep
grep('DIV', fdata)
grep('C', fdata)
fdata[1,]
grep('A', fdata)
fdata[, 12]
ftrain(length(ftrain),)
fdata(length(fdata),)
fdata
fdata[length(fdata),]
fdata[length(fdata)+1,]
fdata[length(fdata),]
length(fdata)
dim(fdata)
dim(fdata)[1]
fdata[dim(fdata)[1],]
library('caret')   # learning
library('rattle')  # plotting of trees
# parallel computing to utilize multiple cores
library(doParallel)
registerDoParallel(cores=4)
# load the data, interpreting certain strings as NaNs:
nans <- c("NA", "", "#DIV/0!")
bench <- read.csv('pml-testing.csv', header=T, na.strings=nans)
fdata <- read.csv('pml-training.csv', header=T, na.strings=nans)
### separate numeric columns
cols <- colnames(fdata)
cols_numeric_bool <- sapply(cols, function(col){is.numeric(fdata[,col])})
cols_numeric <- cols[cols_numeric_bool == T]
cols_factor <- cols[-cols_numeric_bool == F]
cols_numeric
length(cols_numeric)
cols_factor
cols_factor
fdata[, 'user_name']
unique(fdata[, 'user_name'])
fdata[, 'user_name']
fdata[, 'cvtd_timestamp']
fdata[, 'new_window']
fdata[, 'kurtosis_yaw_belt']
as.factor(fdata[, 'kurtosis_yaw_belt'])
fdata[, 'kurtosis_yaw_belt']
fdata[,'skewness_yaw_dumbbell']
is.na(fdata[,'skewness_yaw_dumbbell'])
~is.na(fdata[,'skewness_yaw_dumbbell'])
@is.na(fdata[,'skewness_yaw_dumbbell'])
!is.na(fdata[,'skewness_yaw_dumbbell'])
fdata[!is.na(fdata[,'skewness_yaw_dumbbell'])]
fdata[!is.na(fdata[,'skewness_yaw_dumbbell']), ]
fdata[!is.na(fdata[,'skewness_yaw_dumbbell']), ]
colSums
?colSums
data.table
colSums(is.na(bench))<nrow(bench)
fdata[, 'amplitude_yaw_belt']
is.na
sapply(cols, function(col){is.numeric(fdata[,col])})
sapply(cols, function(col){is.na(fdata[,col])})
cols[colSums(is.na(bench))<nrow(bench)]
cols[colSums(is.na(bench))>nrow(bench)]
cols[colSums(is.na(bench))==nrow(bench)]
#cols[colSums(is.na(bench))==nrow(bench)]
cols[colSums(is.na(fdata))==nrow(fdata)]
cols <- colnames(fdata)
cols_allna <- cols[colSums(is.na(fdata))==nrow(fdata)]
cols_allna
cols <- colnames(fdata)
cols_allna <- cols[colSums(is.na(fdata))==nrow(fdata)]
cols_allna
# 1. drop any columns where ALL values are NaNs
cols <- colnames(fdata)
cols_allna <- cols[colSums(is.na(fdata))==nrow(fdata)]
print('Dropping the following all-NA columns')
print(cols_allna)
cols <- cols[-cols_allna]
cols
length(cols)
# 1. drop any columns where ALL values are NaNs
cols <- colnames(fdata)
cols_allna <- cols[colSums(is.na(fdata))==nrow(fdata)]
print('Dropping the following all-NA columns')
print(cols_allna)
length(cols)
cols_allna
cols[cols_allna]
cols[[cols_allna]]
cols_allna
class(cols)
?subset
subset(cols, cols_allna)
subset(cols, cols_allna)
cols %in% cols_allna
cols %not in% cols_allna
cols %in% cols_allna
!(cols %in% cols_allna)
cols[!(cols %in% cols_allna)]
# 1. drop any columns where ALL values are NaNs
cols_all <- colnames(fdata)
cols_allna <- cols_all[colSums(is.na(fdata))==nrow(fdata)]
cols <- cols_all[!(cols_all %in% cols_allna)]
print length(cols)
length(cols)
print("hello %d", 1)
printf("hello %d", 1)
?print
print(cat('Dropped ', length(cols_allna), ' columns'))
cat(cols_allna)
print(cat('Dropped', length(cols_allna), 'columns:', cols_allna))
# 1. drop any columns where ALL values are NaNs
cols_all <- colnames(fdata)
cols_allna <- cols_all[colSums(is.na(fdata))==nrow(fdata)]
cols <- cols_all[!(cols_all %in% cols_allna)]
print(cat('Dropped', length(cols_allna), 'columns:', cols_allna))
# 2. separate numeric and factor columns
cols_numeric_bool <- sapply(cols, function(col){is.numeric(fdata[,col])})
cols_numeric <- cols[cols_numeric_bool == T]
cols_factor <- cols[-cols_numeric_bool == F]
cols_factor
rowSums
rowSums(is.na(fdata))
rowSums(is.na(fdata)) == ncol(fdata)
fdata[rowSums(is.na(fdata)) == ncol(fdata),]
fdata[, cols]
train[, colnames(train) %in% cols]
fdata[, colnames(fdata) %in% cols]
dim(fdata[, colnames(fdata) %in% cols])
colnames(fdata) %in% cols
cols
c(cols)
bench
colnames(bench)
library('caret')   # learning
library('rattle')  # plotting of trees
# parallel computing to utilize multiple cores
library(doParallel)
registerDoParallel(cores=4)
# load the data, interpreting certain strings as NaNs:
nans <- c("NA", "", "#DIV/0!")
pml_training <- read.csv('pml-training.csv', header=T, na.strings=nans)
pml_testing <- read.csv('pml-testing.csv', header=T, na.strings=nans)
# 1. filter out the columns where ALL values are NaNs
cols_all <- colnames(pml_training)
cols_allna <- cols_all[colSums(is.na(pml_training))==nrow(pml_training)]
cols <- cols_all[!(cols_all %in% cols_allna)]
print(cat('Dropped', length(cols_allna), 'columns:', cols_allna))
# 2. separate numeric and factor columns
cols_numeric_bool <- sapply(cols, function(col){is.numeric(pml_training[,col])})
cols_numeric <- cols[cols_numeric_bool == T]
cols_factor <- cols[-cols_numeric_bool == F]
# 3. apply "good column" filter to training and testing (aka benchmark) data
ftrain <- pml_training[, colnames(pml_training) %in% cols]
fbench <- pml_testing[, colnames(pml_testing) %in% c(cols, 'problem_id')]
c(cols)
########### Visualization ##############
# dependent variables
qplot(fdata$classe, geom="histogram", main='Distribution of dependent variable', xlab='classe', ylab='Counts')
nearZeroVar(fdata, saveMetrics = T, allowParallel=T)
nzv <- nearZeroVar(pml_training[c, cols], saveMetrics = T, allowParallel=T)
pml_training[c, cols]
pml_training[, cols]
nzv <- nearZeroVar(pml_training[, cols], saveMetrics = T, allowParallel=T)
nzv
nzv[, 'zeroVar']
cols[nzv[, 'zeroVar']]
# load the data, interpreting certain strings as NaNs:
nans <- c("NA", "", "#DIV/0!")
pml_training <- read.csv('pml-training.csv', header=T, na.strings=nans)
pml_testing <- read.csv('pml-testing.csv', header=T, na.strings=nans)
# 1. filter out the columns where ALL values are NaNs
cols_all <- colnames(pml_training)
cols_allna <- cols_all[colSums(is.na(pml_training))==nrow(pml_training)]
print(cat('Dropped', length(cols_allna), 'columns (all-NA):', cols_allna))
cols <- cols_all[!(cols_all %in% cols_allna)]
# 2. filter out columns with no variable (these provide little separation power between outcomes)
nzv <- nearZeroVar(pml_training[, cols], saveMetrics = T, allowParallel=T)
cols_novar <- cols[nzv[, 'zeroVar']]
print(cat('Dropped', length(cols_novar), 'columns (zero variance):', cols_novar))
cols <- cols[!(cols %in% cols_novar)]
# 3. drop bad columns from both the training and testing (aka benchmark) datasets
ftrain <- pml_training[, colnames(pml_training) %in% cols]
fbench <- pml_testing[, colnames(pml_testing) %in% c(cols, 'problem_id')]
# 4. separate numeric and factor columns (for later use)
cols_numeric_bool <- sapply(cols, function(col){is.numeric(pml_training[,col])})
cols_numeric <- cols[cols_numeric_bool == T]
cols_factor <- cols[-cols_numeric_bool == F]
########### Visualization ##############
# dependent variables
qplot(fdata$classe, geom="histogram", main='Distribution of dependent variable', xlab='classe', ylab='Counts')
qplot(ftrain$classe, geom="histogram", main='Distribution of dependent variable', xlab='classe', ylab='Counts')
preProcess(ftrain,method="pca",pcaComp=3)
preProcess(ftrain[, cols_numeric], method="pca",pcaComp=3)
df_pca <- preProcess(ftrain[, cols_numeric], method="pca",pcaComp=3)
df_pca
str(df_pca)
df_pca <- preProcess(ftrain[, cols_numeric], method="pca",thresh=0.95)
df_pca <- preProcess(ftrain[, cols_numeric], method="pca",thresh=0.95)
df_pca
df_pca <- preProcess(ftrain[, cols_numeric], method="pca",thresh=0.99)
df_pca
# split full dataset (fdata) into train/val/test using 60-20-20 fractions
set.seed(1)
p6 <- createDataPartition(fdata$classe, p=0.6, list=F)
train <- fdata[p6, ]  # 60%
tv <- fdata[-p6, ]    # 40%
p5 <- createDataPartition(tv$classe, p=0.5, list=F)
val <- tv[p5, ]       # 0.5*40% = 20%
test <- tv[-p5, ]     # 0.5*40% = 20%
get_model_01 <- function(save_mode = F) {
# random forest
name <- 'model.v01.rf.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rf')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_02 <- function(save_mode = F) {
# decision tree
name <- 'model.v02.rpart.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rpart')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_03 <- function(save_mode = F) {
# boosted decision tree
name <- 'model.v03.gbm.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='gbm')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_01 <- function(save_mode = F) {
# random forest
name <- 'model.v01.rf.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rf')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_02 <- function(save_mode = F) {
# decision tree
name <- 'model.v02.rpart.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rpart')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_03 <- function(save_mode = F) {
# boosted decision tree
name <- 'model.v03.gbm.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='gbm')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
train_all_models <- function() {
get_model_01(save_mode=T)
get_model_02(save_mode=T)
get_model_03(save_mode=T)
}
library('caret')   # learning
library('rattle')  # plotting of trees
# parallel computing to utilize multiple cores
library(doParallel)
registerDoParallel(cores=4)
# load the data, interpreting certain strings as NaNs:
nans <- c("NA", "", "#DIV/0!")
pml_training <- read.csv('pml-training.csv', header=T, na.strings=nans)
pml_testing <- read.csv('pml-testing.csv', header=T, na.strings=nans)
# 1. filter out the columns where ALL values are NaNs
cols_all <- colnames(pml_training)
cols_allna <- cols_all[colSums(is.na(pml_training))==nrow(pml_training)]
print(cat('Dropped', length(cols_allna), 'columns (only NA values):', cols_allna))
cols <- cols_all[!(cols_all %in% cols_allna)]
# 2. filter out columns with no variable (these provide little separation power between outcomes)
nzv <- nearZeroVar(pml_training[, cols], saveMetrics = T, allowParallel=T)
cols_novar <- cols[nzv[, 'zeroVar']]
print(cat('Dropped', length(cols_novar), 'columns (zero variance):', cols_novar))
cols <- cols[!(cols %in% cols_novar)]
# 3. drop bad columns from both the training and testing (aka benchmark) datasets
ftrain <- pml_training[, colnames(pml_training) %in% cols]
fbench <- pml_testing[, colnames(pml_testing) %in% c(cols, 'problem_id')]
# 4. separate numeric and factor columns (for later use)
cols_numeric_bool <- sapply(cols, function(col){is.numeric(pml_training[,col])})
cols_numeric <- cols[cols_numeric_bool == T]
cols_factor <- cols[-cols_numeric_bool == F]
########### Visualization ##############
# dependent variables
qplot(ftrain$classe, geom="histogram", main='Distribution of dependent variable', xlab='classe', ylab='Counts')
# Use PCA analysis to extract highest-variance predictors so that we can visualize them
df_pca <- preProcess(ftrain[, cols_numeric], method="pca",thresh=0.99)
# TODO - consider applying PCA: reduce down to 75 vars!
# split full dataset (ftrain) into train/val/test using 60-20-20 fractions
set.seed(1)
p6 <- createDataPartition(ftrain$classe, p=0.6, list=F)
train <- ftrain[p6, ]  # 60%
tv <- ftrain[-p6, ]    # 40%
p5 <- createDataPartition(tv$classe, p=0.5, list=F)
val <- tv[p5, ]       # 0.5*40% = 20%
test <- tv[-p5, ]     # 0.5*40% = 20%
get_model_01 <- function(save_mode = F) {
# random forest
name <- 'model.v01.rf.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rf')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_02 <- function(save_mode = F) {
# decision tree
name <- 'model.v02.rpart.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rpart')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_03 <- function(save_mode = F) {
# boosted decision tree
name <- 'model.v03.gbm.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='gbm')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
train_all_models <- function() {
get_model_01(save_mode=T)
get_model_02(save_mode=T)
get_model_03(save_mode=T)
}
train_all_models()
mod1 <- get_model_01()
mod2 <- get_model_02()
mod3 <- get_model_03()
mod1
predict(mod1, train)
train
dim(train)
dim(predict(mod1, train))
length(predict(mod1, train))
predict(mod1, train)
predict(mod2, train)
predict(mod3, train)
predict(mod3, train)[1111]
predict(mod3, train)
mod1
mod1$modelInfo
mod1$modelInfo()
mod1$results
mod1$finalModel
predict(mod1, train)
predict(mod1, val)
predict(mod1, val)
dim(val)
predict(mod1, val[1,])
predict(mod1, val[2,])
predict(mod1, val[2:10,])
predict(mod1, val[2:1000,])
predict(mod1, val[2:1000,])
predict(mod1, newdata=val[2:1000,])
predict(mod1, newdata=val[2:3,])
val[2:3,]
val[2:3,-classe]
val[2:3,-c('classe')]
val[2:3,c('classe')]
val[2:3,c('classe')]
val[2:3,]
fancyRpartPlot(mod1$finalModel)
fancyRpartPlot(mod2$finalModel)
test
test[1,]
predict(mod1, newdata=test[1,])
predict(mod2, newdata=test[1,])
predict(mod3, newdata=test[1,])
# add pre-processing:
# preProcess=c("center", "scale")
get_model_01 <- function(save_mode = F) {
# random forest
name <- 'model.v01.rf.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rf',
trControl = trainControl(method = "oob"))
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_02 <- function(save_mode = F) {
# decision tree
name <- 'model.v02.rpart.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='rpart')
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
get_model_03 <- function(save_mode = F) {
# boosted decision tree
name <- 'model.v03.gbm.all.rds'
if (save_mode == T) {
set.seed(1)
mod <- train(classe~., data=train, method='gbm',
trControl = trainControl(method = "cv", number=5))
saveRDS(mod, name)
} else {
mod <- readRDS(name)
}
mod
}
train_all_models <- function() {
get_model_01(save_mode=T)
get_model_02(save_mode=T)
get_model_03(save_mode=T)
}
train_all_models()
